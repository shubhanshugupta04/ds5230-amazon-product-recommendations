{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DS 5230 - Unsupervised Machine Learning and Data Mining\n",
        "### Content Based Recommender Systems\n",
        "#### Author - Shubhanshu Gupta"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) Reading json data in PySpark and cleaning it "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install num2words\n",
        "pip install wordninja"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from google.colab import files\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql.functions import isnan, when, count, col, lit, trim, avg, ceil\n",
        "from pyspark.sql.types import StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd\n",
        "from num2words import num2words\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from contractions import CONTRACTION_MAP\n",
        "from nltk.tokenize import word_tokenize\n",
        "import wordninja\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy\n",
        "import sklearn\n",
        "import random\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from textwrap import wrap\n",
        "import matplotlib.gridspec as gridspec\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "import pickle as pk\n",
        "from gensim import corpora, models, similarities\n",
        "import gensim\n",
        "from ast import literal_eval\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.applications import vgg16, resnet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import Model\n",
        "from scipy import sparse"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://usmlproject.s3.amazonaws.com/Electronics.json -O electronics.json\n",
        "!wget https://usmlproject.s3.amazonaws.com/meta_Electronics.json -O metadata.json"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "df = sqlContext.read.json(\"electronics.json\")\n",
        "df.printSchema()\n",
        "final_df = df.select(\"asin\",\"overall\",\"reviewerID\")\n",
        "final_df.show(10)\n",
        "final_df.repartition(1).write.format('com.databricks.spark.csv').save(\"/content/ratings.csv\",header = 'true')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = sqlContext.read.json(\"metadata.json\")\n",
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = ['also_buy','also_view','similar_item','tech1','tech2','fit','details','rank']\n",
        "df = df.drop(*cols_to_drop)\n",
        "dup_df = df.drop_duplicates(subset=['asin'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dup_df = dup_df.withColumn(\"price\", f.regexp_replace(f.col(\"price\"), \"[\\$]\", \"\"))\n",
        "dup_df = dup_df.withColumn(\"price\", dup_df[\"price\"].cast(FloatType()))\n",
        "dup_df = dup_df.withColumn(\"main_cat\", f.regexp_replace(f.col(\"main_cat\"), \"&amp;\", \"&\"))\n",
        "dup_df.coalesce(1).write.format('json').save('/content/metadata.json')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Preprocessing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessing:\n",
        "    \n",
        "    unit_lst = ['g','kg','lbs','lb','oz','mm','cm','km','m','ft','in','inch','ml','kw','j','kj']\n",
        "    \n",
        "    ## Removing html elemnets from the descriptions and features\n",
        "    def cleanhtml(self,raw_html):\n",
        "        cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "        cleantext = re.sub(cleanr, '', raw_html)\n",
        "        return cleantext\n",
        "    \n",
        "    # Removing punctuations from the text\n",
        "    def remove_punctuations(self,text):\n",
        "        text = re.sub(r'[^a-zA-z\\s]', '', text)\n",
        "        return text\n",
        "    \n",
        "    # Removing all special_characters except english alphabets with option to remove digits\n",
        "    def remove_special_characters(self,text, remove_digits=False):\n",
        "        pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "        text = re.sub(pattern, '', text)\n",
        "        return text\n",
        "    \n",
        "    # Convert numbers into words\n",
        "    def convert_text(self,text):   \n",
        "        txt = text.split(' ')\n",
        "        for i in range(len(txt)):\n",
        "            if txt[i].replace('.','',1).isdigit():\n",
        "                txt[i] = num2words(txt[i])\n",
        "        return ' '.join(txt)\n",
        "    \n",
        "    # Remove single letters like x b etc.\n",
        "    def remove_single_letters(self,text):\n",
        "        reg = re.compile('(?:^| )[b-hj-z](?= |$)')\n",
        "        text = re.sub(reg, '', text)\n",
        "        return text\n",
        "      \n",
        "    # expandcontractions such as isn't to is not\n",
        "    def expand_contractions(self,text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "        def expand_match(contraction):\n",
        "            match = contraction.group(0)\n",
        "            first_char = match[0]\n",
        "            expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "            expanded_contraction = first_char+expanded_contraction[1:]\n",
        "            return expanded_contraction\n",
        "        \n",
        "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "        return expanded_text\n",
        "    \n",
        "    # A lot of units are used so to remove common units\n",
        "    def remove_units(self,text):\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [token.strip() for token in tokens]\n",
        "        filtered_tokens = [token for token in tokens if token not in unit_lst]     \n",
        "        filtered_text = ' '.join(filtered_tokens)    \n",
        "        return filtered_text\n",
        "    \n",
        "    # Splitting joined english words like 'themoney' to 'the money'\n",
        "    def split_words(self,text):\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [token.strip() for token in tokens]\n",
        "        filtered_tokens = [' '.join(wordninja.split(token)) for token in tokens]        \n",
        "        filtered_text = ' '.join(filtered_tokens)    \n",
        "        return filtered_text\n",
        "    \n",
        "    def text_preprocessing(self,text):\n",
        "        text = self.cleanhtml(text)\n",
        "        text = self.expand_contractions(text)\n",
        "        text = self.convert_text(text)\n",
        "        text = self.split_words(text)\n",
        "        text = self.remove_single_letters(text)\n",
        "        text = self.remove_units(text)\n",
        "        text = self.remove_special_characters(text,True)\n",
        "        # removing extra whitespace\n",
        "        text = re.sub(' +', ' ', text)\n",
        "        # stripping extra space\n",
        "        text = text.strip()\n",
        "        text = text.lower()\n",
        "        return text\n",
        "    \n",
        "text_processing = TextProcessing()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "product_df = pd.read_json(\"metadata.json\", lines=True)\n",
        "product_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Since description and features are in string object so correcting it\n",
        "product_df['asin'] = product_df['asin'].astype('str')\n",
        "product_df['description'] = product_df['description'].apply(lambda s: s[1:-1])\n",
        "product_df['feature'] = product_df['feature'].apply(lambda s: s[1:-1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning description and feature data\n",
        "product_df['description'] = product_df['description'].apply(lambda text: text_processing.text_preprocessing(text))\n",
        "product_df['feature'] = product_df['feature'].apply(lambda text: text_processing.text_preprocessing(text))\n",
        "product_df['text'] = product_df['description'] + ' ' + product_df['feature']\n",
        "product_df.to_csv('product_desc.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rating_df = pd.read_csv(\"ratings.csv\")\n",
        "rating_df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing distribution of ratings in the user review data\n",
        "sns.countplot(x=\"overall\", data=rating_df, palette=\"Set3\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 category of products in electronics dataset\n",
        "cat_df = product_df[\"main_cat\"].value_counts(sort = True)[:10].to_frame()\n",
        "cat_df = cat_df.reset_index()\n",
        "sns.set(style=\"darkgrid\")\n",
        "g = sns.barplot(x=\"main_cat\", y=\"index\", data=cat_df)\n",
        "for index, row in cat_df.iterrows():\n",
        "  g.text(row.main_cat,index, row.main_cat, color='black', ha=\"left\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 brands based on number of products in electronics dataset\n",
        "brand_df = product_df[(product_df[\"brand\"] != '') & (product_df[\"brand\"] != 'Generic')]['brand'].value_counts(sort = True)[:10].to_frame()\n",
        "brand_df = brand_df.reset_index()\n",
        "g = sns.barplot(x=\"brand\", y=\"index\", data=brand_df)\n",
        "for index, row in brand_df.iterrows():\n",
        "  g.text(row.brand,index, row.brand, color='black', ha=\"left\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Review per category\n",
        "full_df = rating_df.set_index('asin').join(product_df.set_index('asin'))\n",
        "review_per_cat = full_df.groupby(\"main_cat\").count()\n",
        "rev_df = review_per_cat.sort_values('overall', ascending=False)[:10].reset_index()\n",
        "g = sns.barplot(x=\"overall\", y=\"main_cat\", data=rev_df)\n",
        "for index, row in rev_df.iterrows():\n",
        "  g.text(row.overall,index, row.overall, color='black', ha=\"left\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 1 : Content based Recommender based on only features and descriptions of products"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## As the data is a lot decreasing it to manageable form for recommendations\n",
        "## Keeping only top 5 categories found in EDA\n",
        "products = product_df[['description','feature','text','brand','main_cat','title','image']]\n",
        "products['no_of_images'] = products['image'].apply(lambda x: len(x))\n",
        "temp = products[\"main_cat\"].value_counts(sort = True)[:5].index\n",
        "temp = temp.to_list()\n",
        "products = products[(products['main_cat'].isin(temp))]\n",
        "products = products[(products['no_of_images'] == 5)]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing url to get full image\n",
        "def change_url(x):\n",
        "  lst = []\n",
        "  for im in x:\n",
        "    lst.append(re.sub(\"\\.[^.]*((?=.(jpg|jpeg|png|gif)))\", \"\", im)) \n",
        "  return lst\n",
        "products['image'] = products['image'].apply(lambda x: change_url(x))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving final product file\n",
        "products.reset_index(inplace=True)\n",
        "products.to_csv('amazon_product_final.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only rows in user which have the products in our small product catalog and keeping users with more than 5 ratings to remove cold start problem\n",
        "user_df = rating_df[rating_df['asin'].isin(products['asin'])]\n",
        "users_interactions_count_df = user_df.groupby(['reviewerID', 'asin']).size().groupby('reviewerID').size()\n",
        "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['reviewerID']]\n",
        "print(f'Users with at least 5 interactions: {len(users_with_enough_interactions_df)}')\n",
        "interactions_from_selected_users_df = user_df.merge(users_with_enough_interactions_df, \n",
        "               how = 'right',\n",
        "               left_on = 'reviewerID',\n",
        "               right_on = 'reviewerID')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving final user data\n",
        "interactions_from_selected_users_df.to_csv('amazon_user_final.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Feature based Recommender"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def display_img(images,title,content):\n",
        "  fig = plt.figure(figsize=(20,4))\n",
        "  gs = gridspec.GridSpec(1, 6)\n",
        "  gs.update(hspace=0.05, wspace=0.05)\n",
        "  cnt = 0\n",
        "\n",
        "  for i in range(len(images)):\n",
        "    ax = plt.subplot(gs[i])\n",
        "    cnt = cnt + 1\n",
        "    response = requests.get(images[i])\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    ax.set_title(\"\\n\".join(wrap(title[i], 20)),{'fontsize':15})\n",
        "    ax.axis('off')\n",
        "    ax.text(0.5,-0.1,\"\\n\".join(wrap(content[i], 20)), fontsize=15,transform=ax.transAxes,horizontalalignment='center',verticalalignment='center')\n",
        "    ax.imshow(img)\n",
        "  plt.show()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def content_recommender(prod_asin,topn, matrix = tfidf_matrix,model = \"vectorizer\"):\n",
        "  product_images = []\n",
        "  title = []\n",
        "  content = []\n",
        "\n",
        "  orig_product = product_features[product_features.index == prod_asin]\n",
        "  product_images.append(orig_product.iloc[0]['image'])\n",
        "  title.append(orig_product.iloc[0]['title'])\n",
        "  content.append(f\"Brand: {orig_product.iloc[0]['brand']}\")\n",
        "\n",
        "  index = indices[prod_asin]\n",
        "\n",
        "  if model == \"vectorizer\":\n",
        "    cosin_similarity = cosine_similarity(matrix[index:index + 1], matrix).flatten()\n",
        "    sim_scores = list(enumerate(cosin_similarity))\n",
        "  elif model == \"lda\":\n",
        "    sim_scores = list(enumerate(cos_similarities[matrix[index]]))\n",
        "  elif model == \"word2vec\":\n",
        "    sim_scores = list(enumerate(cos_similarities_word[index]))\n",
        "  elif model == \"cnn\":\n",
        "    sim_scores = list(enumerate(cosSimilarities[index]))\n",
        "\n",
        "\n",
        "  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "  sim_scores = sim_scores[1:topn + 1]\n",
        "  product_indices = [(indices.index[i[0]],i[1]) for i in sim_scores]\n",
        "  recommendations_df = pd.DataFrame(product_indices, columns=['asin', 'recStrength'])\n",
        "\n",
        "  items = product_features.reset_index()\n",
        "  recommendations_df = recommendations_df.merge(items, how = 'left', \n",
        "                                                          left_on = 'asin', \n",
        "                                                          right_on = 'asin')[['recStrength', 'asin', 'title', 'image','brand']]\n",
        "  recommendations_df = recommendations_df.sort_values('recStrength',ascending = False)\n",
        "\n",
        "  for index, row in recommendations_df[0:5].iterrows():\n",
        "        product_images.append(row['image'])\n",
        "        title.append(row['title'])\n",
        "        content.append(f\"Brand: {row['brand']} \\n Similarity: {round(row['recStrength'],3)}\")\n",
        "\n",
        "  display_img(product_images,title,content)\n",
        "  return recommendations_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "products = pd.read_csv(\"amazon_product_final.csv\",index_col=0, converters={'image': literal_eval})\n",
        "products.set_index('asin',inplace=True)\n",
        "products.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "product_features = products.copy()\n",
        "product_features['image'] = product_features['image'].apply(lambda x: x[0])\n",
        "product_features.reset_index(inplace = True)\n",
        "product_features.drop([21916,25449,37774,39105],inplace=True)\n",
        "product_features.set_index('asin',inplace = True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf vectorizer\n",
        "vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                     ngram_range=(1, 2),\n",
        "                     max_features=20000,\n",
        "                     stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(product_features['text'].apply(lambda x: np.str_(x)))\n",
        "tfidf_feature_name = vectorizer.get_feature_names()\n",
        "tfidf_matrix.shape\n",
        "\n",
        "pk.dump(vectorizer,open('tf_idf_vectorizer.pkl','wb'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "ind = []\n",
        "for i in product_features.index:\n",
        "  ind.append(count)\n",
        "  count = count + 1\n",
        "indices = pd.Series(ind, index=product_features.index)\n",
        "indices"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df = content_recommender('B01HDB1SJU',20)\n",
        "tfidf_score = tfidf_df['recStrength'].values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Vectorizer Feature based recommender"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "countVectorizer = CountVectorizer(analyzer='word',ngram_range=(1, 2),\n",
        "                     max_features=20000,\n",
        "                     stop_words='english')\n",
        "count_matrix = countVectorizer.fit_transform(product_features['text'].apply(lambda x: np.str_(x)))\n",
        "count_feature_name = countVectorizer.get_feature_names()\n",
        "count_matrix.shape\n",
        "pk.dump(countVectorizer,open('count_vectorizer.pkl','wb'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "count_df = content_recommender('B01HDB1SJU',20,matrix=count_matrix)\n",
        "count_score = count_df['recStrength'].values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2vec Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://usmlproject.s3.amazonaws.com/GoogleNews-vectors-negative300.bin.gz -O GoogleNews-vectors-negative300.bin.gz"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Training word2vec on our corpus\n",
        "corpus = []\n",
        "for index,words in enumerate(product_features['text']):\n",
        "  corpus.append(words.split(' '))\n",
        "\n",
        "google_word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "# Training our corpus with Google Pretrained Model\n",
        "google_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)\n",
        "google_model.build_vocab(corpus)\n",
        "google_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True)\n",
        "google_model.train(corpus, total_examples=google_model.corpus_count, epochs = 5)\n",
        "pk.dump(google_model,open('word2vecModel.pkl','wb'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = []\n",
        "\n",
        "def vectors(product_features):\n",
        "\n",
        "  # Creating a list for storing the vectors (description into vectors)\n",
        "  global word_embeddings\n",
        "  # word_embeddings = []\n",
        "\n",
        "  # Reading the each book description \n",
        "  for line in product_features['text']:\n",
        "    featureVec = np.zeros((300,), dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    for word in line.split():\n",
        "      nwords += 1\n",
        "      if word in google_model.wv.vocab:\n",
        "        featureVec = np.add(featureVec, google_model.wv[word])\n",
        "    if(nwords>0):\n",
        "        featureVec = np.divide(featureVec, nwords)\n",
        "    word_embeddings.append(featureVec)\n",
        "  word_embeddings = np.array(word_embeddings)\n",
        "\n",
        "vectors(product_features)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarities_word = cosine_similarity(word_embeddings, word_embeddings)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_df = content_recommender('B01HDB1SJU',20,model = \"word2vec\")\n",
        "word2vec_score = word2vec_df['recStrength'].values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA Feature Based Recommender"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "def convert_to_token():\n",
        "    texts = []\n",
        "    for text in product_features['text']:\n",
        "        tokens = word_tokenize(text)\n",
        "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "        texts.append(filtered_tokens)\n",
        "    return texts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "product_text = convert_to_token()\n",
        "dictionary = corpora.Dictionary(product_text)\n",
        "corpus = [dictionary.doc2bow(txt) for txt in product_text]\n",
        "print(f'Number of unique tokens: {len(dictionary)}') \n",
        "print(f'Number of articles:{len(corpus)}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model on different number of topics\n",
        "topicnums = [10,15,20,25,30,35,40,45,50]\n",
        "ldamodels_bow = {}\n",
        "for i in topicnums:\n",
        "    random.seed(42)\n",
        "    ldamodels_bow[i] = models.LdaModel(corpus, num_topics=i, random_state=42, update_every=1, passes=10, id2word=dictionary)\n",
        "    pk.dump(ldamodels_bow[i],open('ldamodels_bow_'+str(i)+'.lda','wb'))    \n",
        "    print(f'ldamodels_bow_{i}.lda created.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding similarity between different topics\n",
        "def jaccard_similarity(query, document):\n",
        "    intersection = set(query).intersection(set(document))\n",
        "    union = set(query).union(set(document))\n",
        "    return float(len(intersection))/float(len(union))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lda_topics = {}\n",
        "for i in topicnums:\n",
        "    lda_model = pk.load(open('ldamodels_bow_'+str(i)+'.lda','rb'))\n",
        "    lda_topics_string = lda_model.show_topics(i)\n",
        "    lda_topics[i] = [\"\".join([c if c.isalpha() else \" \" for c in topic[1]]).split() for topic in lda_topics_string]\n",
        "\n",
        "lda_stability = {}\n",
        "for i in range(0,len(topicnums)-1):\n",
        "    jacc_sims = []\n",
        "    for t1,topic1 in enumerate(lda_topics[topicnums[i]]):\n",
        "        sims = []\n",
        "        for t2,topic2 in enumerate(lda_topics[topicnums[i+1]]):\n",
        "            sims.append(jaccard_similarity(topic1,topic2))    \n",
        "        jacc_sims.append(sims)    \n",
        "    lda_stability[topicnums[i]] = jacc_sims"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mean_stability = [np.array(lda_stability[i]).mean() for i in topicnums[:-1]]\n",
        "\n",
        "with sns.axes_style(\"darkgrid\"):\n",
        "    x = topicnums[:-1]\n",
        "    y = mean_stability\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.plot(x,y,label='Average Overlap Between Topics')\n",
        "    plt.xlim([1, 55])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.xlabel('Number of topics')\n",
        "    plt.ylabel('Average Jaccard similarity')   \n",
        "    plt.title('Average Jaccard Similarity Between Topics')\n",
        "    plt.legend()    \n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 25\n",
        "lda_model_final = pk.load(open('ldamodels_bow_'+str(num_topics)+'.lda','rb'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarities = similarities.MatrixSimilarity(lda_model_final[corpus])\n",
        "corpus_lda_model = lda_model_final[corpus]\n",
        "cos_similarities[corpus_lda_model[0]].shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lda_df = content_recommender('B01HDB1SJU',20,matrix=corpus_lda_model,model = \"lda\")\n",
        "lda_scores = lda_df['recStrength'].values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image based feature recommendations"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://usmlproject.s3.amazonaws.com/train_images.zip\n",
        "!unzip train_images.zip"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height, _ = 224, 224, 3 #load_image(df.iloc[0].image).shape\n",
        "\n",
        "# Pre-Trained Model\n",
        "base_model = vgg16.VGG16(weights='imagenet', \n",
        "                      include_top=False, \n",
        "                      input_shape = (img_width, img_height, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add Layer Embedding\n",
        "vgg_model = keras.Sequential([\n",
        "    base_model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "vgg_model.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(model, img_name):\n",
        "    # Reshape\n",
        "    img = image.load_img(img_path(img_name), target_size=(img_width, img_height))\n",
        "    # img to Array\n",
        "    x   = image.img_to_array(img)\n",
        "    # Expand Dim (1, w, h)\n",
        "    x   = np.expand_dims(x, axis=0)\n",
        "    # Pre process Input\n",
        "    x   = preprocess_input(x)\n",
        "    return model.predict(x).reshape(-1)\n",
        "\n",
        "def img_path(img):\n",
        "    return \"train_images/\"+img"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "map_embeddings = product_features['image'].apply(lambda img: get_embedding(vgg_model, img))\n",
        "df_embs_vgg = map_embeddings.apply(pd.Series)\n",
        "\n",
        "print(df_embs_vgg.shape)\n",
        "df_embs_vgg.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cosSimilarities = cosine_similarity(df_embs_vgg,df_embs_vgg)\n",
        "cosSimilarities"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_df = content_recommender('B01HDB1SJU',20, model = \"cnn\")\n",
        "vgg16_score = vgg16_df['recStrength'].values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing cosine similarities of all models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cosin_distance=[]\n",
        "num_results=20\n",
        "cosin_distance.append(sum(tfidf_score)/num_results)\n",
        "cosin_distance.append(sum(count_score)/num_results)\n",
        "cosin_distance.append(sum(lda_scores)/num_results)\n",
        "cosin_distance.append(sum(word2vec_score)/num_results)\n",
        "cosin_distance.append(sum(cnn_score)/num_results)\n",
        "\n",
        "x=cosin_distance\n",
        "y=[]\n",
        "for i in range(0,10,2):\n",
        "    y.append(i)\n",
        "    \n",
        "objects = ('tf_idf', 'bag_of_words', 'lda','avg_w2v', 'cnn')\n",
        "y_pos = np.arange(len(objects))\n",
        "plt.plot(y,x)\n",
        "plt.xticks(y, objects)\n",
        "plt.ylabel('Cosine Distance')\n",
        "plt.title('Cosine Distance Measurement')\n",
        " \n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Plotting barplot\n",
        "plt.bar(objects,x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 2: Based on Item and User profiles"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  \n",
        "EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n",
        "\n",
        "### Get the items which user has already interacted with\n",
        "def get_items_interacted(user_id, interactions_df):\n",
        "    interacted_items = interactions_from_selected_users_df.loc[user_id]['asin']\n",
        "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])\n",
        "\n",
        "orig = []\n",
        "pred = []\n",
        "\n",
        "### Evaluate model on different RMSE\n",
        "class ModelEvaluator:\n",
        "\n",
        "  ## Get items which user has not interacted with\n",
        "  def get_not_interacted_items_sample(self, user_id, sample_size, seed=42):\n",
        "      interacted_items = get_items_interacted(user_id, interactions_from_selected_users_df)\n",
        "      all_items = set(product_features.index)\n",
        "      non_interacted_items = all_items - interacted_items\n",
        "\n",
        "      random.seed(seed)\n",
        "      non_interacted_items_sample = random.sample(non_interacted_items, sample_size)\n",
        "      return set(non_interacted_items_sample)\n",
        "\n",
        "  def RMSE(self,orig,pred):\n",
        "    return sqrt(mean_squared_error(orig, pred))\n",
        "\n",
        "  # calculate similarity of items for each user and each item\n",
        "  def get_item_similarity(self,user_id,user_product_ids,item_id,min=0.1):\n",
        "    \n",
        "    if item_id in user_product_ids:\n",
        "      return interactions_from_selected_users_df[(interactions_from_selected_users_df.index == user_id) & (interactions_from_selected_users_df.asin == item_id)]['overall']\n",
        "    \n",
        "    index = indices[item_id]\n",
        "    cosin_similarity = cosine_similarity(matrix[index:index + 1], matrix).flatten()\n",
        "    sim_scores = list(enumerate(cosin_similarity))\n",
        "    sim_scores = [(x,y) for x,y in sim_scores if y > min]\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:]\n",
        "    product_indices = [(indices.index[i[0]],i[1]) for i in sim_scores]\n",
        "    recommendations_df = pd.DataFrame(product_indices, columns=['asin', 'recStrength'])\n",
        "    recommendations_df = recommendations_df[recommendations_df['asin'].isin(user_product_ids)]\n",
        "\n",
        "    sim_num = 0.0\n",
        "    sim_denom = 0.0\n",
        "    rating = 0.0\n",
        "    try:\n",
        "\n",
        "      if len(recommendations_df) > 0:\n",
        "        for index,row in recommendations_df.iterrows():\n",
        "          user_rating = interactions_from_selected_users_df[(interactions_from_selected_users_df.index == user_id) & (interactions_from_selected_users_df.asin == row['asin'])]['overall'].values[0]\n",
        "          sim_num += row['recStrength'] * user_rating\n",
        "          sim_denom += np.abs(row['recStrength'])\n",
        "\n",
        "        rating = sim_num/sim_denom\n",
        "    except:\n",
        "      print(recommendations_df)\n",
        "      print(user_id)\n",
        "\n",
        "    return rating\n",
        "\n",
        "\n",
        "  ## Evaluate recommender for each user\n",
        "  def evaluate_model_for_user(self, model, user_id):\n",
        "\n",
        "        #Getting the items in test set\n",
        "      interacted_values_testset = interactions_test_df.loc[user_id]\n",
        "      \n",
        "      if type(interacted_values_testset['asin']) == pd.Series:\n",
        "          user_interacted_items_testset = set(interacted_values_testset['asin'])\n",
        "          user_rating_testset = interacted_values_testset['overall']\n",
        "      else:\n",
        "          user_interacted_items_testset = set([interacted_values_testset['asin']])  \n",
        "          user_rating_testset = [interacted_values_testset['overall']]\n",
        "      \n",
        "      interacted_items_count_testset = len(user_interacted_items_testset) \n",
        "\n",
        "        #Getting a ranked recommendation list from a model for a given user\n",
        "      user_recs_df = model.recommend_items(user_id, \n",
        "                                               items_to_ignore=get_items_interacted(user_id, \n",
        "                                                                                    interactions_train_df), \n",
        "                                               topn=100)\n",
        "\n",
        "      user_product_ids = interactions_train_df[interactions_train_df.index == user_id]['asin']\n",
        "        #For each item the user has interacted in test set\n",
        "      global orig\n",
        "      global pred\n",
        "\n",
        "      for i,item_id in enumerate(user_interacted_items_testset):\n",
        "          pred_rating = self.get_item_similarity(user_id,user_product_ids.tolist(),item_id)\n",
        "          orig.append(user_rating_testset[i])\n",
        "          pred.append(pred_rating)\n",
        "            \n",
        "            \n",
        "  ## Evaluate model for all the users  \n",
        "  def evaluate_model(self, model):\n",
        "      global orig\n",
        "      global pred\n",
        "      print(orig)\n",
        "      user_metrics = []\n",
        "      for idx, user_id in tqdm(enumerate(list(interactions_test_df.index.unique().values))):\n",
        "          self.evaluate_model_for_user(model, user_id)  \n",
        "      print('%d users processed' % idx)\n",
        "        \n",
        "      pred1 = pred.copy()\n",
        "      for i,x in enumerate(pred1):\n",
        "            if type(x) == pd.Series:\n",
        "                pred1[i] = x.values[0]\n",
        "        \n",
        "      global_metrics = {'rmse':self.RMSE(orig,pred1)}   \n",
        "                           \n",
        "      return global_metrics\n",
        "    \n",
        "model_evaluator = ModelEvaluator()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## class to create profile of users\n",
        "class UserProfile:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.indices = indices      \n",
        "        \n",
        "    def get_item_profile(self,item_id): \n",
        "      idx = indices[item_id]\n",
        "      item_profile = matrix[idx:idx+1]\n",
        "      return item_profile\n",
        "\n",
        "    def get_item_profiles(self,ids):\n",
        "      item_profiles_list = [self.get_item_profile(x) for x in ids]\n",
        "      #print(item_profiles_list)\n",
        "      item_profiles = scipy.sparse.vstack(item_profiles_list)\n",
        "      #print(item_profiles)\n",
        "      return item_profiles\n",
        "\n",
        "    def build_users_profile(self,user_id, interactions_indexed_df):\n",
        "        interactions_user_df = interactions_indexed_df.loc[user_id]\n",
        "        user_item_profiles = self.get_item_profiles(interactions_user_df['asin'])  \n",
        "        user_item_strengths = np.array(interactions_user_df['overall']).reshape(-1,1)\n",
        "        user_item_strengths_weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths), axis=0) / np.sum(user_item_strengths)\n",
        "        user_profile_norm = sklearn.preprocessing.normalize(user_item_strengths_weighted_avg)\n",
        "        return user_profile_norm\n",
        "\n",
        "    def build_users_profiles(self): \n",
        "        interactions_indexed_df = interactions_train_df[interactions_train_df['asin'] \\\n",
        "                                                   .isin(product_features.index)]\n",
        "        user_profiles = {}\n",
        "        for user_id in interactions_indexed_df.index.unique():\n",
        "            user_profiles[user_id] = self.build_users_profile(user_id, interactions_indexed_df)\n",
        "        return user_profiles\n",
        "\n",
        "user_profile = UserProfile()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# content based recommender class\n",
        "class ContentBasedRecommender:\n",
        "    \n",
        "    def __init__(self, matrix,items_df=None):\n",
        "        self.item_ids = indices\n",
        "        self.matrix = matrix\n",
        "        self.items_df = items_df\n",
        "        \n",
        "        \n",
        "    def _get_similar_items_to_user_profile(self, user_id, topn=10):\n",
        "        #Computes the cosine similarity between the user profile and all item profiles\n",
        "        cosine_similarities = cosine_similarity(user_profiles[user_id], self.matrix)\n",
        "        #Gets the top similar items\n",
        "        similar_indices = cosine_similarities.argsort().flatten()[-topn:]\n",
        "        #Sort the similar items by similarity\n",
        "        similar_items = sorted([(indices[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1])\n",
        "        return similar_items\n",
        "        \n",
        "    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n",
        "        similar_items = self._get_similar_items_to_user_profile(user_id)\n",
        "        #Ignores items the user has already interacted\n",
        "        similar_items_filtered = list(filter(lambda x: x[0] not in items_to_ignore, similar_items))\n",
        "        sim_scores = sorted(similar_items_filtered, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[0:topn]\n",
        "\n",
        "        product_indices = [(indices.index[i[0]],i[1]) for i in sim_scores]\n",
        "        \n",
        "        recommendations_df = pd.DataFrame(product_indices, columns=['asin', 'recStrength'])\n",
        "\n",
        "        return recommendations_df\n",
        "content_based_recommender_model = ContentBasedRecommender(matrix,product_features)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating for different methods"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) TF-IDF"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = tfidf_matrix\n",
        "user_profiles = user_profile.build_users_profiles()\n",
        "len(user_profiles)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_evaluator.evaluate_model(content_based_recommender_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Bag Of Words"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = count_matrix\n",
        "user_profiles = user_profile.build_users_profiles()\n",
        "len(user_profiles)\n",
        "model_evaluator.evaluate_model(content_based_recommender_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Average Word2Vec"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = sparse.csr_matrix(word_embeddings)\n",
        "user_profiles = user_profile.build_users_profiles()\n",
        "len(user_profiles)\n",
        "model_evaluator.evaluate_model(content_based_recommender_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Transfer Learning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = sparse.csr_matrix(df_embs)\n",
        "user_profiles = user_profile.build_users_profiles()\n",
        "len(user_profiles)\n",
        "model_evaluator.evaluate_model(content_based_recommender_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "nteract": {
      "version": "0.27.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}